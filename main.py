# main.py
import os
import argparse
# --- ìˆ˜ì •ëœ ë¶€ë¶„: ëª¨ë“  ëª¨ë“ˆì„ 'modules' í´ë”ì—ì„œ ê°€ì ¸ì˜¤ë„ë¡ ë³€ê²½ ---
from modules import config
from modules.crawler import RecipeCrawler
from modules.preprocess import DataPreprocessor
from modules.vector_store import VectorStoreManager
from modules.retriever import AdvancedRetriever
from modules.llm_handler import LLMHandler
# -------------------------------------------------------------
import sys
import pysqlite3
# ë‚´ì¥ sqlite3 ëª¨ë“ˆì„ pysqlite3ë¡œ ë®ì–´ì“°ê¸°
sys.modules["sqlite3"] = pysqlite3

# --- ì¶”ê°€/ìˆ˜ì •ëœ ë¶€ë¶„ ---
def main(rebuild_db: bool, until_step: str):
    """
    QA ì—”ì§„ì˜ ì „ì²´ ì‹¤í–‰ íë¦„ì„ ì œì–´í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜.
    """
    # 1. í¬ë¡¤ë§
    if not os.path.exists(config.CRAWLED_DATA_DIR) or not os.listdir(config.CRAWLED_DATA_DIR):
        print("--- 1. ë°ì´í„° í¬ë¡¤ë§ ì‹œì‘ ---")
        os.makedirs(config.CRAWLED_DATA_DIR, exist_ok=True)
        crawler = RecipeCrawler()
        crawler.run(start_page=1, end_page=3, output_filename=os.path.join(config.CRAWLED_DATA_DIR, "baek_recipes_1-3.json"))
        crawler.run(start_page=11, end_page=20, output_filename=os.path.join(config.CRAWLED_DATA_DIR, "baek_recipes_11-20.json"))
        crawler.run(start_page=21, end_page=30, output_filename=os.path.join(config.CRAWLED_DATA_DIR, "baek_recipes_21-30.json"))
        crawler.run(start_page=31, end_page=40, output_filename=os.path.join(config.CRAWLED_DATA_DIR, "baek_recipes_31-40.json"))
        crawler.run(start_page=41, end_page=50, output_filename=os.path.join(config.CRAWLED_DATA_DIR, "baek_recipes_41-50.json"))
        crawler.run(start_page=51, end_page=60, output_filename=os.path.join(config.CRAWLED_DATA_DIR, "baek_recipes_51-60.json"))
    else:
        print(f"--- 1. í¬ë¡¤ë§ ê±´ë„ˆë›°ê¸° ('{config.CRAWLED_DATA_DIR}' í´ë”ì— íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤) ---")

    # 'crawl' ë‹¨ê³„ê¹Œì§€ë§Œ ì‹¤í–‰í•˜ëŠ” ì˜µì…˜ í™•ì¸
    if until_step == 'crawl':
        print("\nSUCCESS: 'crawl' ë‹¨ê³„ê¹Œì§€ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    # 2. ë°ì´í„° ì „ì²˜ë¦¬
    preprocessing_needed = not os.path.exists(config.MERGED_PREPROCESSED_FILE)
    if preprocessing_needed:
        print("\n--- 2. ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ---")
        preprocessor = DataPreprocessor()
        success = preprocessor.run(config.CRAWLED_DATA_DIR, config.MERGED_PREPROCESSED_FILE)
        
        if not success:
            print("CRITICAL: ë°ì´í„° ì „ì²˜ë¦¬ì— ì‹¤íŒ¨í•˜ì—¬ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
    else:
        print(f"--- 2. ì „ì²˜ë¦¬ ê±´ë„ˆë›°ê¸° ('{config.MERGED_PREPROCESSED_FILE}' íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤) ---")

    # 'preprocess' ë‹¨ê³„ê¹Œì§€ë§Œ ì‹¤í–‰í•˜ëŠ” ì˜µì…˜ í™•ì¸
    if until_step == 'preprocess':
        print("\nSUCCESS: 'preprocess' ë‹¨ê³„ê¹Œì§€ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    # --- ì´í•˜ ì½”ë“œëŠ” until_step == 'run' ì¼ ë•Œë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤. ---
    
    # 3. ë²¡í„° DB êµ¬ì¶• ë˜ëŠ” ë¡œë“œ
    print("\n--- 3. ë²¡í„° DB ì¤€ë¹„ ì‹œì‘ ---")
    vs_manager = VectorStoreManager()
    
    if rebuild_db or not os.path.exists(config.CHROMA_DB_PATH):
        if rebuild_db: print("INFO: --rebuild-db ì˜µì…˜ì— ë”°ë¼ DBë¥¼ ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.")
        vectorstore = vs_manager.build(json_path=config.MERGED_PREPROCESSED_FILE)
    else:
        vectorstore = vs_manager.load()
    
    if not vectorstore:
        print("CRITICAL: ë²¡í„° DB ì¤€ë¹„ì— ì‹¤íŒ¨í•˜ì—¬ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # 4. Advanced RAG ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
    print("\n--- 4. RAG ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ---")
    docs_for_retriever = vs_manager._load_documents_from_json(config.MERGED_PREPROCESSED_FILE)
    adv_retriever = AdvancedRetriever(vectorstore)
    retriever = adv_retriever.get_retriever()
    if docs_for_retriever:
        retriever.add_documents(docs_for_retriever)
        print("INFO: ParentDocumentRetriever ì„¤ì • ì™„ë£Œ.")
    else:
        print("WARNING: ë¦¬íŠ¸ë¦¬ë²„ì— ì¶”ê°€í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

    # 5. LLM í•¸ë“¤ëŸ¬ ë° RAG ì²´ì¸ ìƒì„±
    print("\n--- 5. QA ì—”ì§„(LLM) ì´ˆê¸°í™” ---")
    llm_handler = LLMHandler(retriever=retriever)
    qa_chain = llm_handler.create_rag_chain()
    print("SUCCESS: ë°±ì¢…ì› ë ˆì‹œí”¼ QA ì—”ì§„ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")

    # 6. ëŒ€í™”í˜• QA ì„¸ì…˜ ì‹œì‘
    print("\n--- ì•ˆë…•í•˜ì„¸ìš”! ë°±ì£¼ë¶€ì…ë‹ˆë‹¤. ë­ë“  ë¬¼ì–´ë³´ì…”ìœ  (ì¢…ë£Œí•˜ë ¤ë©´ 'ê·¸ë§Œ') ---")
    session_id = "user_session_01" 
    
    while True:
        try:
            user_input = input("ğŸ¤” ì§ˆë¬¸: ")
            if user_input.lower() == 'ê·¸ë§Œ':
                print("\në‹¤ìŒì— ë˜ ì°¾ì•„ì£¼ì…”ìœ ! ë§›ìˆê²Œ í•´ë“œì„¸ìœ ~")
                break
            
            response = qa_chain.invoke(
                {"input": user_input},
                config={"configurable": {"session_id": session_id}}
            )
            
            print("\në°±ì£¼ë¶€ ğŸ’¬:")
            print(response['answer'])
            print("-" * 50)
            
        except KeyboardInterrupt:
            print("\në‹¤ìŒì— ë˜ ì°¾ì•„ì£¼ì…”ìœ ! ë§›ìˆê²Œ í•´ë“œì„¸ìœ ~")
            break
        except Exception as e:
            print(f"\nERROR: ì£„ì†¡í•´ìœ , ì²˜ë¦¬ ì¤‘ì— ë¬¸ì œê°€ ìƒê²¼ì–´ìœ : {e}")

if __name__ == '__main__':
    # --- ì¶”ê°€/ìˆ˜ì •ëœ ë¶€ë¶„: ì‹¤í–‰ ì˜µì…˜ ì¶”ê°€ ---
    parser = argparse.ArgumentParser(description="ë°±ì¢…ì› ë ˆì‹œí”¼ QA ì—”ì§„")
    parser.add_argument(
        '--rebuild-db',
        action='store_true',
        help="ê¸°ì¡´ ë²¡í„° DBë¥¼ ë¬´ì‹œí•˜ê³  ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤."
    )
    parser.add_argument(
        '--until-step',
        type=str,
        choices=['crawl', 'preprocess', 'run'],
        default='run',
        help="ì‹¤í–‰í•  ë§ˆì§€ë§‰ ë‹¨ê³„ë¥¼ ì§€ì •í•©ë‹ˆë‹¤: crawl, preprocess, run (ì „ì²´ ì‹¤í–‰)"
    )
    args = parser.parse_args()
    
    main(rebuild_db=args.rebuild_db, until_step=args.until_step)
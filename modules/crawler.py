import requests
from bs4 import BeautifulSoup
import time
import json
import random

class RecipeCrawler:
    """
    ì¬ë£Œ ì¶”ì¶œ ë¡œì§ì„ ëŒ€í­ ê°•í™”í•˜ì—¬ ëª¨ë“  ì¬ë£Œë¥¼ ì •í™•í•˜ê²Œ í¬ë¡¤ë§í•˜ëŠ” ë²„ì „.
    """
    def __init__(self):
        self.base_url = "https://www.10000recipe.com"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36'
        }

    # í˜ì´ì§€ êµ¬ê°„ì„ ì •í•´ì„œ ë°±ì¢…ì› ë ˆì‹œí”¼ URLì„ ê°€ì ¸ì˜¤ëŠ” ê¸°ëŠ¥ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
    def get_baek_recipe_urls(self, start_page=1, end_page=5):
        recipe_urls = []
        print(f"'ë°±ì¢…ì›' ê²€ìƒ‰ ê²°ê³¼ {start_page}í˜ì´ì§€ë¶€í„° {end_page}í˜ì´ì§€ê¹Œì§€ URL ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        for page in range(start_page, end_page + 1):
            search_url = f"{self.base_url}/recipe/list.html?q=%EB%B0%B1%EC%A2%85%EC%9B%90&page={page}"
            try:
                response = requests.get(search_url, headers=self.headers, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                links = soup.select('li.common_sp_list_li a.common_sp_link')
                
                if not links:
                    print(f"{page} í˜ì´ì§€ì—ì„œ ë” ì´ìƒ ë ˆì‹œí”¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break

                for link in links:
                    recipe_urls.append(link['href'])
                
                print(f"  > {page} í˜ì´ì§€ì—ì„œ {len(links)}ê°œì˜ URL ìˆ˜ì§‘ ì™„ë£Œ.")
                time.sleep(random.uniform(1, 2))

            except requests.exceptions.RequestException as e:
                print(f"  > {page} í˜ì´ì§€ ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue
        
        unique_urls = list(set(recipe_urls))
        print(f"\nì´ {len(unique_urls)}ê°œì˜ ê³ ìœ í•œ ë ˆì‹œí”¼ URLì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        return unique_urls

    # --- ğŸ‘‡ ì—¬ê¸°ê°€ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ì…ë‹ˆë‹¤! ğŸ‘‡ ---
    def scrape_recipe_details(self, recipe_url):
        """ê°œë³„ ë ˆì‹œí”¼ URLë¡œ ì ‘ì†í•´ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. (ì¬ë£Œ ì¶”ì¶œ ë¡œì§ ê°•í™”)"""
        full_url = self.base_url + recipe_url
        try:
            response = requests.get(full_url, headers=self.headers, timeout=10)
            if response.status_code != 200:
                return None

            soup = BeautifulSoup(response.text, 'html.parser')

            # ì œëª© ì¶”ì¶œ (ê¸°ì¡´ê³¼ ë™ì¼)
            title_element = soup.select_one('div.view2_summary h3, div.view2_summary h2')
            title = title_element.get_text(strip=True) if title_element else ""

            # ğŸ¯ ì¬ë£Œ ì¶”ì¶œ ë¡œì§ ê°œì„ 
            ingredients = []
            ingredient_area = soup.select_one('div#divConfirmedMaterialArea')
            if ingredient_area:
                # <ul> íƒœê·¸ë¥¼ ëª¨ë‘ ì°¾ì•„ì„œ ìˆœíšŒ
                ingredient_uls = ingredient_area.find_all('ul')
                for ul in ingredient_uls:
                    # ê° <ul> ì•ˆì— ìˆëŠ” <li> íƒœê·¸ë“¤ì„ ìˆœíšŒ
                    items = ul.find_all('li')
                    for item in items:
                        # <li> íƒœê·¸ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¨ í›„, ë¶ˆí•„ìš”í•œ 'êµ¬ë§¤' ë²„íŠ¼ í…ìŠ¤íŠ¸ì™€ ê³µë°± ì œê±°
                        full_text = item.get_text(separator=' ').strip()
                        # 'êµ¬ë§¤' ë¼ëŠ” ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê·¸ ì•ê¹Œì§€ë§Œ ì‚¬ìš©
                        if 'êµ¬ë§¤' in full_text:
                            clean_text = full_text.split('êµ¬ë§¤')[0].strip()
                        else:
                            clean_text = full_text
                        
                        # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šê³ , ì¬ë£Œ ê·¸ë£¹ ì œëª©('[ì¬ë£Œ]', '[ì–‘ë…]' ë“±)ì´ ì•„ë‹ˆë©´ ì¶”ê°€
                        if clean_text and not clean_text.startswith('['):
                            ingredients.append(clean_text)

            # ì¡°ë¦¬ ìˆœì„œ ì¶”ì¶œ (ê¸°ì¡´ê³¼ ë™ì¼)
            step_elements = soup.select('div.view_step_cont.media div.media-body')
            steps = []
            if step_elements:
                for i, elem in enumerate(step_elements):
                    step_text = elem.get_text(strip=True)
                    if step_text:
                        steps.append(f"ë‹¨ê³„ {i+1}: {step_text}")

            if not title or not ingredients or not steps:
                return None

            recipe_id = recipe_url.split('/')[-1]
            return {
                'id': recipe_id,
                'title': title,
                'ingredients': ', '.join(ingredients), # ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨
                'steps': ' '.join(steps),
                'url': full_url
            }
        except Exception as e:
            print(f"  > '{recipe_url}' íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def run(self, start_page=1, end_page=5, output_filename='baek_recipes.json'):
        """'ë°±ì¢…ì›' ë ˆì‹œí”¼ í¬ë¡¤ë§ ì „ì²´ ê³¼ì •ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        recipe_urls = self.get_baek_recipe_urls(start_page, end_page)
        
        if not recipe_urls:
            print("ìˆ˜ì§‘ëœ URLì´ ì—†ì–´ í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return

        all_recipes = []
        total_count = len(recipe_urls)
        print(f"\nì´ {total_count}ê°œì˜ ë ˆì‹œí”¼ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

        for i, url in enumerate(recipe_urls):
            print(f"({i+1}/{total_count}) '{url}' í¬ë¡¤ë§ ì¤‘...")
            details = self.scrape_recipe_details(url)
            
            if details:
                all_recipes.append(details)
            
            time.sleep(random.uniform(1, 1.5))

        if not all_recipes:
            print("\ní¬ë¡¤ë§ ê²°ê³¼, ìœ íš¨í•œ ë ˆì‹œí”¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(all_recipes, f, ensure_ascii=False, indent=4)
        print(f"\ní¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(all_recipes)}ê°œì˜ ë°±ì¢…ì› ë ˆì‹œí”¼ë¥¼ '{output_filename}' íŒŒì¼ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")